{
  "title": "Models",
  "$ref": "#/definitions/continuedev__core__models__Models",
  "definitions": {
    "BaseCompletionOptions": {
      "title": "BaseCompletionOptions",
      "type": "object",
      "properties": {
        "temperature": {
          "title": "Temperature",
          "description": "The temperature of the completion.",
          "type": "number"
        },
        "top_p": {
          "title": "Top P",
          "description": "The top_p of the completion.",
          "type": "number"
        },
        "top_k": {
          "title": "Top K",
          "description": "The top_k of the completion.",
          "type": "integer"
        },
        "presence_penalty": {
          "title": "Presence Penalty",
          "description": "The presence penalty Aof the completion.",
          "type": "number"
        },
        "frequency_penalty": {
          "title": "Frequency Penalty",
          "description": "The frequency penalty of the completion.",
          "type": "number"
        },
        "stop": {
          "title": "Stop",
          "description": "The stop tokens of the completion.",
          "type": "array",
          "items": {
            "type": "string"
          }
        },
        "max_tokens": {
          "title": "Max Tokens",
          "description": "The maximum number of tokens to generate.",
          "default": 1023,
          "type": "integer"
        }
      }
    },
    "RequestOptions": {
      "title": "RequestOptions",
      "type": "object",
      "properties": {
        "timeout": {
          "title": "Timeout",
          "description": "Set the timeout for each request to the LLM. If you are running a local LLM that takes a while to respond, you might want to set this to avoid timeouts.",
          "default": 300,
          "type": "integer"
        },
        "verify_ssl": {
          "title": "Verify Ssl",
          "description": "Whether to verify SSL certificates for requests.",
          "type": "boolean"
        },
        "ca_bundle_path": {
          "title": "Ca Bundle Path",
          "description": "Path to a custom CA bundle to use when making the HTTP request",
          "type": "string"
        },
        "proxy": {
          "title": "Proxy",
          "description": "Proxy URL to use when making the HTTP request",
          "type": "string"
        },
        "headers": {
          "title": "Headers",
          "description": "Headers to use when making the HTTP request",
          "type": "object",
          "additionalProperties": {
            "type": "string"
          }
        }
      }
    },
    "LLM": {
      "title": "LLM",
      "type": "object",
      "properties": {
        "title": {
          "title": "Title",
          "description": "A title that will identify this model in the model selection dropdown",
          "type": "string"
        },
        "unique_id": {
          "title": "Unique Id",
          "description": "The unique ID of the user.",
          "type": "string"
        },
        "model": {
          "title": "Model",
          "description": "The name of the model to be used (e.g. gpt-4, codellama)",
          "type": "string"
        },
        "system_message": {
          "title": "System Message",
          "description": "A system message that will always be followed by the LLM",
          "type": "string"
        },
        "context_length": {
          "title": "Context Length",
          "description": "The maximum context length of the LLM in tokens, as counted by count_tokens.",
          "default": 2048,
          "type": "integer"
        },
        "completion_options": {
          "title": "Completion Options",
          "description": "Options for the completion endpoint. Read more about the completion options in the documentation.",
          "default": {
            "temperature": null,
            "top_p": null,
            "top_k": null,
            "presence_penalty": null,
            "frequency_penalty": null,
            "stop": null,
            "max_tokens": 1023
          },
          "allOf": [
            {
              "$ref": "#/definitions/BaseCompletionOptions"
            }
          ]
        },
        "request_options": {
          "title": "Request Options",
          "description": "Options for the HTTP request to the LLM.",
          "default": {
            "timeout": 300,
            "verify_ssl": null,
            "ca_bundle_path": null,
            "proxy": null,
            "headers": null
          },
          "allOf": [
            {
              "$ref": "#/definitions/RequestOptions"
            }
          ]
        },
        "prompt_templates": {
          "title": "Prompt Templates",
          "description": "A dictionary of prompt templates that can be used to customize the behavior of the LLM in certain situations. For example, set the \"edit\" key in order to change the prompt that is used for the /edit slash command. Each value in the dictionary is a string templated in mustache syntax, and filled in at runtime with the variables specific to the situation. See the documentation for more information.",
          "default": {},
          "type": "object"
        },
        "api_key": {
          "title": "Api Key",
          "description": "The API key for the LLM provider.",
          "type": "string"
        },
        "api_base": {
          "title": "Api Base",
          "description": "The base URL of the LLM API.",
          "type": "string"
        }
      },
      "required": [
        "model"
      ]
    },
    "continuedev__core__models__Models": {
      "title": "Models",
      "description": "Main class that holds the current model configuration",
      "type": "object",
      "properties": {
        "default": {
          "$ref": "#/definitions/LLM"
        },
        "summarize": {
          "$ref": "#/definitions/LLM"
        },
        "edit": {
          "$ref": "#/definitions/LLM"
        },
        "chat": {
          "$ref": "#/definitions/LLM"
        },
        "saved": {
          "title": "Saved",
          "default": [],
          "type": "array",
          "items": {
            "$ref": "#/definitions/LLM"
          }
        },
        "temperature": {
          "title": "Temperature",
          "type": "number"
        },
        "system_message": {
          "title": "System Message",
          "type": "string"
        }
      },
      "required": [
        "default",
        "summarize",
        "edit",
        "chat"
      ]
    }
  }
}