{
  "title": "ModelDescription",
  "$ref": "#/definitions/continuedev__core__config__ModelDescription",
  "definitions": {
    "BaseCompletionOptions": {
      "title": "BaseCompletionOptions",
      "type": "object",
      "properties": {
        "temperature": {
          "title": "Temperature",
          "description": "The temperature of the completion.",
          "type": "number"
        },
        "top_p": {
          "title": "Top P",
          "description": "The top_p of the completion.",
          "type": "number"
        },
        "top_k": {
          "title": "Top K",
          "description": "The top_k of the completion.",
          "type": "integer"
        },
        "presence_penalty": {
          "title": "Presence Penalty",
          "description": "The presence penalty Aof the completion.",
          "type": "number"
        },
        "frequency_penalty": {
          "title": "Frequency Penalty",
          "description": "The frequency penalty of the completion.",
          "type": "number"
        },
        "stop": {
          "title": "Stop",
          "description": "The stop tokens of the completion.",
          "type": "array",
          "items": {
            "type": "string"
          }
        },
        "max_tokens": {
          "title": "Max Tokens",
          "description": "The maximum number of tokens to generate.",
          "default": 1023,
          "type": "integer"
        }
      }
    },
    "RequestOptions": {
      "title": "RequestOptions",
      "type": "object",
      "properties": {
        "timeout": {
          "title": "Timeout",
          "description": "Set the timeout for each request to the LLM. If you are running a local LLM that takes a while to respond, you might want to set this to avoid timeouts.",
          "default": 300,
          "type": "integer"
        },
        "verify_ssl": {
          "title": "Verify Ssl",
          "description": "Whether to verify SSL certificates for requests.",
          "type": "boolean"
        },
        "ca_bundle_path": {
          "title": "Ca Bundle Path",
          "description": "Path to a custom CA bundle to use when making the HTTP request",
          "type": "string"
        },
        "proxy": {
          "title": "Proxy",
          "description": "Proxy URL to use when making the HTTP request",
          "type": "string"
        },
        "headers": {
          "title": "Headers",
          "description": "Headers to use when making the HTTP request",
          "type": "object",
          "additionalProperties": {
            "type": "string"
          }
        }
      }
    },
    "continuedev__core__config__ModelDescription": {
      "title": "ModelDescription",
      "type": "object",
      "properties": {
        "title": {
          "title": "Title",
          "description": "The title you wish to give your model.",
          "type": "string"
        },
        "provider": {
          "title": "Provider",
          "description": "The provider of the model. This is used to determine the type of model, and how to interact with it.",
          "enum": [
            "openai",
            "free-trial",
            "openai-aiohttp",
            "anthropic",
            "bedrock",
            "together",
            "ollama",
            "huggingface-tgi",
            "huggingface-inference-api",
            "llama.cpp",
            "replicate",
            "text-gen-webui",
            "google-palm",
            "lmstudio",
            "llamafile"
          ],
          "type": "string"
        },
        "model": {
          "title": "Model",
          "description": "The name of the model. Used to autodetect prompt template.",
          "type": "string"
        },
        "api_key": {
          "title": "Api Key",
          "description": "OpenAI, Anthropic, Together, or other API key",
          "type": "string"
        },
        "api_base": {
          "title": "Api Base",
          "description": "The base URL of the LLM API.",
          "type": "string"
        },
        "context_length": {
          "title": "Context Length",
          "description": "The maximum context length of the LLM in tokens, as counted by count_tokens.",
          "default": 2048,
          "type": "integer"
        },
        "template": {
          "title": "Template",
          "description": "The chat template used to format messages. This is auto-detected for most models, but can be overridden here.",
          "enum": [
            "llama2",
            "alpaca",
            "zephyr",
            "phind",
            "anthropic",
            "chatml",
            "deepseek",
            "neural-chat"
          ],
          "type": "string"
        },
        "completion_options": {
          "title": "Completion Options",
          "description": "Options for the completion endpoint. Read more about the completion options in the documentation.",
          "default": {
            "temperature": null,
            "top_p": null,
            "top_k": null,
            "presence_penalty": null,
            "frequency_penalty": null,
            "stop": null,
            "max_tokens": 1023
          },
          "allOf": [
            {
              "$ref": "#/definitions/BaseCompletionOptions"
            }
          ]
        },
        "system_message": {
          "title": "System Message",
          "description": "A system message that will always be followed by the LLM",
          "type": "string"
        },
        "request_options": {
          "title": "Request Options",
          "description": "Options for the HTTP request to the LLM.",
          "default": {
            "timeout": 300,
            "verify_ssl": null,
            "ca_bundle_path": null,
            "proxy": null,
            "headers": null
          },
          "allOf": [
            {
              "$ref": "#/definitions/RequestOptions"
            }
          ]
        }
      },
      "required": ["title", "provider", "model"]
    }
  }
}
