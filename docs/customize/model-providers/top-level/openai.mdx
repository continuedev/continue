---
title: "How to Configure OpenAI Models with Continue"
slug: ../openai
description: "Configure OpenAI models with Continue, including GPT-4o for chat and text-embedding-3-large for embeddings, with support for OpenAI-compatible servers and APIs"
sidebarTitle: "OpenAI"
---

<Info>
  You can get an API key from the [OpenAI
  console](https://platform.openai.com/account/api-keys)
</Info>

## How to Set Up OpenAI Chat Models

We recommend configuring **GPT-4o** as your chat model.

<Tabs>
  <Tab title="YAML">
  ```yaml title="config.yaml"
  models:
    - name: GPT-4o
      provider: openai
      model: gpt-4o
      apiKey: <YOUR_OPENAI_API_KEY>
  ```
  </Tab>
  <Tab title="JSON">
  ```json title="config.json"
  {
    "models": [
      {
        "title": "GPT-4o",
        "provider": "openai", 
        "model": "gpt-4o",
        "apiKey": "<YOUR_OPENAI_API_KEY>"
      }
    ]
  }
  ```
  </Tab>
</Tabs>

## OpenAI Autocomplete Model Availability

OpenAI currently does not offer any autocomplete models.

[Click here](../../model-roles/autocomplete.md) to see a list of autocomplete model providers.

## How to Configure OpenAI Embeddings Models

We recommend configuring **text-embedding-3-large** as your embeddings model.

<Tabs>
  <Tab title="YAML">
  ```yaml title="config.yaml"
  models:
    - name: OpenAI Embeddings
      provider: openai
      model: text-embedding-3-large
      apiKey: <YOUR_OPENAI_API_KEY>
      roles:
        - embed
  ```
  </Tab>
  <Tab title="JSON">
  ```json title="config.json"
  {
    "embeddingsProvider": {
      "provider": "openai",
      "model": "text-embedding-3-large", 
      "apiKey": "<YOUR_OPENAI_API_KEY>"
    }
  }
  ```
  </Tab>
</Tabs>

## OpenAI Reranking Model Availability

OpenAI currently does not offer any reranking models.

[Click here](../../model-roles/reranking.mdx) to see a list of reranking model providers.

## How to Use OpenAI-Compatible Servers and APIs

OpenAI compatible servers

- [KoboldCpp](https://github.com/lostruins/koboldcpp)
- [text-gen-webui](https://github.com/oobabooga/text-generation-webui/tree/main/extensions/openai#setup--installation)
- [FastChat](https://github.com/lm-sys/FastChat/blob/main/docs/openai_api.md)
- [LocalAI](https://localai.io/basics/getting_started/)
- [llama-cpp-python](https://github.com/abetlen/llama-cpp-python#web-server)
- [TensorRT-LLM](https://github.com/NVIDIA/trt-llm-as-openai-windows?tab=readme-ov-file#examples)
- [vLLM](https://docs.vllm.ai/en/latest/serving/openai_compatible_server.html)
- [BerriAI/litellm](https://github.com/BerriAI/litellm)

OpenAI compatible APIs

- [Anyscale Endpoints](https://github.com/continuedev/deploy-os-code-llm#others)
- [Anyscale Private Endpoints](https://github.com/continuedev/deploy-os-code-llm#anyscale-private-endpoints)

### How to Configure Custom API Base for OpenAI-Compatible Services

If you are using an OpenAI compatible server / API, you can change the `apiBase` like this:

<Tabs>
  <Tab title="YAML">
  ```yaml title="config.yaml"
  models:
    - name: OpenAI-compatible server / API
      provider: openai
      model: MODEL_NAME 
      apiBase: http://localhost:8000/v1
      apiKey: <YOUR_CUSTOM_API_KEY>
  ```
  </Tab>
  <Tab title="JSON">
  ```json title="config.json"
  {
    "models": [
      {
        "title": "OpenAI-compatible server / API", 
        "provider": "openai",
        "model": "MODEL_NAME",
        "apiKey": "<YOUR_CUSTOM_API_KEY>",
        "apiBase": "http://localhost:8000/v1"
      }
    ]
  }
  ```
  </Tab>
</Tabs>

### How to Force Legacy Completions Endpoint Usage

To force usage of `chat/completions` instead of `completions` endpoint you can set:

<Tabs>
  <Tab title="YAML">
  ```yaml title="config.yaml"
  models:
    - name: OpenAI-compatible server / API
      provider: openai
      model: MODEL_NAME 
      apiBase: http://localhost:8000/v1
      useLegacyCompletionsEndpoint: true
  ```
  </Tab>
  <Tab title="JSON">
  ```json title="config.json"
  {
    "models": [
      {
        "title": "OpenAI-compatible server / API",
        "provider": "openai",
        "model": "MODEL_NAME",
        "apiBase": "http://localhost:8000/v1", 
        "useLegacyCompletionsEndpoint": true
      }
    ]
  }
  ```
  </Tab>
</Tabs>