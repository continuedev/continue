---
title: Ollama
slug: ../ollama
---

Ollama is an open-source tool that allows to run large language models (LLMs) locally on their own computers. To use Ollama, you can install it [here](https://ollama.ai/download) and download the model you want to run with the `ollama run` command.

## Chat model

We recommend configuring **Llama3.1 8B** as your chat model.

<Tabs>
    <Tab title="YAML">
    ```yaml title="config.yaml"
    models:
      - name: Llama3.1 8B
        provider: ollama
        model: llama3.1:8b
    ```
    </Tab>
    <Tab title="JSON">
    ```json title="config.json"
    {
      "models": [
        {
          "title": "Llama3.1 8B",
          "provider": "ollama",
          "model": "llama3.1:8b"
        }
      ]
    }
    ```
    </Tab>
</Tabs>

## Autocomplete model

We recommend configuring **Qwen2.5-Coder 1.5B** as your autocomplete model.

<Tabs>
    <Tab title="YAML">
    ```yaml title="config.yaml"
    models:
      - name: Qwen2.5-Coder 1.5B
        provider: ollama
        model: qwen2.5-coder:1.5b-base
        roles:
          - autocomplete
    ```
    </Tab>
    <Tab title="JSON">
    ```json title="config.json"
    {
      "tabAutocompleteModel": {
        "title": "Qwen2.5-Coder 1.5B",
        "provider": "ollama",
        "model": "qwen2.5-coder:1.5b-base"
      }
    }
    ```
    </Tab>
</Tabs>

## Embeddings model

We recommend configuring **Nomic Embed Text** as your embeddings model.

<Tabs>
    <Tab title="YAML">
    ```yaml title="config.yaml"
    models:
      - name: Nomic Embed Text
        provider: ollama
        model: nomic-embed-text
        roles:
          - embed
    ```
    </Tab>
    <Tab title="JSON">
    ```json title="config.json"
    {
      "embeddingsProvider": {
        "provider": "ollama",
        "model": "nomic-embed-text"
      }
    }
    ```
    </Tab>
</Tabs>

## Reranking model

Ollama currently does not offer any reranking models.

[Click here](../../model-roles/reranking.mdx) to see a list of reranking model providers.

## Using a remote instance

To configure a remote instance of Ollama, add the `"apiBase"` property to your model in config.json:

<Tabs>
    <Tab title="YAML">
    ```yaml title="config.yaml"
    models:
      - name: Llama3.1 8B
        provider: ollama
        model: llama3.1:8b
        apiBase: http://<my endpoint>:11434
    ```
    </Tab>
    <Tab title="JSON">
    ```json title="config.json"
    {
      "models": [
        {
          "title": "Llama3.1 8B",
          "provider": "ollama",
          "model": "llama3.1:8b",
          "apiBase": "http://<my endpoint>:11434"
        }
      ]
    }
    ```
    </Tab>
</Tabs>

## Model Capabilities

Ollama models usually have their capabilities auto-detected correctly. However, if you're using custom model names or experiencing issues with tools/images not working, you can explicitly set capabilities:

<Tabs>
    <Tab title="YAML">
    ```yaml title="config.yaml"
    models:
      - name: Custom Vision Model
        provider: ollama
        model: my-custom-llava
        capabilities:
          - tool_use      # Enable if your model supports function calling
          - image_input   # Enable for vision models like llava
    ```
    </Tab>
    <Tab title="JSON">
    ```json title="config.json"
    {
      "models": [
        {
          "title": "Custom Vision Model",
          "provider": "ollama",
          "model": "my-custom-llava",
          "capabilities": {
            "tools": true,
            "uploadImage": true
          }
        }
      ]
    }
    ```
    </Tab>
</Tabs>

<Note>
Most standard Ollama models (like llama3.1, mistral, etc.) support tool use by default. Vision models (like llava) also support image input.
</Note>
