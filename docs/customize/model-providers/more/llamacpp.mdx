---
title: "Llama.cpp"
description: "Configure Llama.cpp with Continue to run local language models using high-performance C++ inference, including setup instructions for the server and client configuration"
---

<Info>
  Get started with [Llama.cpp](https://github.com/ggml-org/llama.cpp?tab=readme-ov-file#quick-start)
</Info>

## Configuration

<Tabs>
    <Tab title="YAML">
    ```yaml title="config.yaml"
    models:
      - name: <MODEL_NAME>
        provider: llama.cpp
        model: <MODEL_ID>
        apiBase: http://localhost:8080
    ```
    </Tab>
    <Tab title="JSON (Deprecated)">
    ```json title="config.json"
    {
      "models": [
        {
          "title": "<MODEL_NAME>",
          "provider": "llama.cpp",
          "model": "<MODEL_ID>"
          "apiBase": "http://localhost:8080"
        }
      ]
    }
    ```
    </Tab>
</Tabs>