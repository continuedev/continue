---
title: Chat Role
description: Chat model role
keywords: [chat, model, role]
sidebar_position: 1
---

import TabItem from "@theme/TabItem";
import Tabs from "@theme/Tabs";

A "chat model" is an LLM that is trained to respond in a conversational format. Because they should be able to answer general questions and generate complex code, the best chat models are typically large, often 405B+ parameters.

In Continue, these models are used for [Chat](../../chat/how-to-use-it.md) and [Actions](../../actions/how-to-use-it.md). The selected chat model will also be used for [Edit](../../edit/how-to-use-it.md) and [Apply](./apply.mdx) if no `edit` or `apply` models are specified, respectively.

## Recommended Chat models

## Best overall experience

For the best overall Chat experience, you will want to use a 400B+ parameter model or one of the frontier models.

### Claude Sonnet 3.5 from Anthropic

Our current top recommendation is Claude 3.7 Sonnet from [Anthropic](../model-providers/top-level/anthropic.mdx).

<Tabs groupId="hub-config-example">
  <TabItem value="hub" label="Hub">
  View the [Claude 3.7 Sonnet model block](https://hub.continue.dev/anthropic/claude-3-7-sonnet) on the hub.
  </TabItem>
  <TabItem value="yaml" label="YAML">
  ```yaml title="config.yaml"
  models:
    - name: Claude 3.7 Sonnet
      provider: anthropic
      model: claude-3-7-sonnet-latest
      apiKey: <YOUR_ANTHROPIC_API_KEY>
  ```
  </TabItem>
  <TabItem value="json" label="JSON">
  ```json title="config.json"
  {
    "models": [
      {
        "title": "Claude 3.5 Sonnet",
        "provider": "anthropic",
        "model": "claude-3-5-sonnet-latest",
        "apiKey": "<YOUR_ANTHROPIC_API_KEY>"
      }
    ]
  }
  ```
  </TabItem>
</Tabs>

### Llama 3.1 405B from Meta

If you prefer to use an open-weight model, then Llama 3.1 405B from Meta is your best option right now. You will need to decide if you use it through a SaaS model provider (e.g. [Together](../model-providers/more/together.mdx) or [Groq](../model-providers/more/groq.mdx)) or self-host it (e.g. using [vLLM](../model-providers//more/vllm.mdx) or [Ollama](../model-providers/top-level/ollama.mdx)).

<Tabs groupId="config-example">
  {/* HUB_TODO nonexistent blocks */}
  {/* <TabItem value="hub" label="Hub">
    <Tabs groupId="providers">
        <TabItem value="Together">
        Add the [Together Llama 3.1 405b block](https://hub.continue.dev/explore/assistants) from the hub
        </TabItem>
         <TabItem value="Groq">
        Add the [Groq Llama 3.1 405b block](https://hub.continue.dev/explore/assistants) from the hub
        </TabItem>
        <TabItem value="vLLM">
        Add the [vLLM Llama 3.1 405b block](https://hub.continue.dev/explore/assistants) from the hub
        </TabItem>
        <TabItem value="Ollama">
        Add the [Ollama Llama 3.1 405b block](https://hub.continue.dev/explore/assistants) from the hub
        </TabItem>
    </Tabs>
  </TabItem> */}
  <TabItem value="yaml" label="YAML">
    <Tabs groupId="providers">
        <TabItem value="Together">
        ```yaml title="config.yaml"
        models:
          - name: "Llama 3.1 405B"
            provider: "together"
            model: "llama3.1-405b"
            apiKey: <YOUR_TOGETHER_API_KEY>
        ```
        </TabItem>
        <TabItem value="Groq">
        ```yaml title="config.yaml"
        models:
          - name: "Llama 3.1 405B"
            provider: "groq"
            model: "llama3.1-405b"
            apiKey: <YOUR_GROQ_API_KEY>
        ```
        </TabItem>
        <TabItem value="vLLM">
        ```yaml title="config.yaml"
        models:
          - name: "Llama 3.1 405B"
            provider: "vllm"
            model: "llama3.1-405b"
        ```
        </TabItem>
        <TabItem value="Ollama">
        ```yaml title="config.yaml"
        models:
          - name: "Llama 3.1 405B"
            provider: "ollama"
            model: "llama3.1:405b"
        ```
        </TabItem>
    </Tabs>
  </TabItem>
  <TabItem value="json" label="JSON">
    <Tabs groupId="providers">
        <TabItem value="Together">
        ```json title="config.json"
        {
          "models": [
            {
              "title": "Llama 3.1 405B",
              "provider": "together",
              "model": "llama3.1-405b",
              "apiKey": "<YOUR_TOGETHER_API_KEY>"
            }
          ]
        }
        ```
        </TabItem>
        <TabItem value="Groq">
        ```json title="config.json"
        {
          "models": [
            {
              "title": "Llama 3.1 405B",
              "provider": "groq",
              "model": "llama3.1-405b",
              "apiKey": "<YOUR_GROQ_API_KEY>"
            }
          ]
        }
        ```
        </TabItem>
        <TabItem value="vLLM">
        ```json title="config.json"
        {
          "models": [
            {
              "title": "Llama 3.1 405B",
              "provider": "vllm",
              "model": "llama3.1-405b"
            }
          ]
        }
        ```
        </TabItem>
        <TabItem value="Ollama">
        ```json title="config.json"
        {
          "models": [
            {
              "title": "Llama 3.1 405B",
              "provider": "ollama",
              "model": "llama3.1:405b"
            }
          ]
        }
        ```
        </TabItem>
    </Tabs>
  </TabItem>
</Tabs>

### GPT-4o from OpenAI

If you prefer to use a model from [OpenAI](../model-providers/top-level/openai.mdx), then we recommend GPT-4o.

<Tabs groupId="hub-config-example">
    <TabItem value="hub" label="Hub">
    Add the [OpenAI GPT-4o block](https://hub.continue.dev/openai/gpt-4o) from the hub
    </TabItem>
  <TabItem value="yaml" label="YAML">
  ```yaml title="config.yaml"
  models:
    - name: GPT-4o
      provider: openai
      model: ''
      apiKey: <YOUR_OPENAI_API_KEY>
  ```
  </TabItem>
  <TabItem value="json" label="JSON">
  ```json title="config.json"
  {
    "models": [
      {
        "title": "GPT-4o",
        "provider": "openai",
        "model": "",
        "apiKey": "<YOUR_OPENAI_API_KEY>"
      }
    ]
  }
  ```
  </TabItem>
</Tabs>

### Grok-2 from xAI

If you prefer to use a model from [xAI](../model-providers/top-level/xAI.mdx), then we recommend Grok-2.

<Tabs groupId="hub-config-example">
    <TabItem value="hub" label="Hub">
    Add the [xAI Grok-2 block](https://hub.continue.dev/xai/grok-2) from the hub
    </TabItem>
  <TabItem value="yaml" label="YAML">
  ```yaml title="config.yaml"
  models:
    - name: Grok-2
      provider: xAI
      model: grok-2-latest
      apiKey: <YOUR_XAI_API_KEY>
  ```
  </TabItem>
  <TabItem value="json" label="JSON">
  ```json title="config.json"
  {
    "models": [
      {
        "title": "Grok-2",
        "provider": "xAI",
        "model": "grok-2-latest",
        "apiKey": "<YOUR_XAI_API_KEY>"
      }
    ]
  }
  ```
  </TabItem>
</Tabs>

### Gemini 2.0 Flash from Google

If you prefer to use a model from [Google](../model-providers/top-level/gemini.mdx), then we recommend Gemini 2.0 Flash.

<Tabs groupId="hub-config-example">
    <TabItem value="hub" label="Hub">
    Add the [Gemini 2.0 Flash block](https://hub.continue.dev/google/gemini-2.0-flash) from the hub
    </TabItem>
  <TabItem value="yaml" label="YAML">
  ```yaml title="config.yaml"
  models:
    - name: Gemini 2.0 Flash
      provider: gemini
      model: gemini-2.0-flash
      apiKey: <YOUR_GEMINI_API_KEY>
  ```
  </TabItem>
  <TabItem value="json" label="JSON">
  ```json title="config.json"
  {
    "models": [
      {
        "title": "Gemini 2.0 Flash",
        "provider": "gemini",
        "model": "gemini-2.0-flash",
        "apiKey": "<YOUR_GEMINI_API_KEY>"
      }
    ]
  }
  ```
  </TabItem>
</Tabs>

## Local, offline experience

For the best local, offline Chat experience, you will want to use a model that is large but fast enough on your machine.

### Llama 3.1 8B

If your local machine can run an 8B parameter model, then we recommend running Llama 3.1 8B on your machine (e.g. using [Ollama](../model-providers/top-level/ollama.mdx) or [LM Studio](../model-providers/more/lmstudio.mdx)).

<Tabs groupId="hub-config-example">
  <TabItem value="hub" label="Hub">
    <Tabs groupId="providers">
        <TabItem value="ollama" label="Ollama">
    Add the [Ollama Llama 3.1 8b block](https://hub.continue.dev/ollama/llama3.1-8b) from the hub
    </TabItem>
    {/* HUB_TODO nonexistent block */}
    {/* <TabItem value="lmstudio" label="LM Studio">
    Add the [LM Studio Llama 3.1 8b block](https://hub.continue.dev/explore/models) from the hub
    </TabItem> */}
    </Tabs>
  </TabItem>
  <TabItem value="yaml" label="YAML">
    <Tabs groupId="providers">
      <TabItem value="Ollama">
      ```yaml title="config.yaml"
      models:
        - name: Llama 3.1 8B
          provider: ollama
          model: llama3.1:8b
      ```
      </TabItem>
      <TabItem value="LM Studio">
      ```yaml title="config.yaml"
      models:
        - name: Llama 3.1 8B
          provider: lmstudio
          model: llama3.1:8b
      ```
      </TabItem>
    </Tabs>
  </TabItem>
  <TabItem value="json" label="JSON">
    <Tabs groupId="providers">
      <TabItem value="Ollama">
      ```json title="config.json"
      {
        "models": [
          {
            "title": "Llama 3.1 8B",
            "provider": "ollama",
            "model": "llama3.1:8b"
          }
        ]
      }
      ```
      </TabItem>
      <TabItem value="LM Studio">
      ```json title="config.json"
      {
        "models": [
          {
            "title": "Llama 3.1 8B",
            "provider": "lmstudio",
            "model": "llama3.1-8b"
          }
        ]
      }
      ```
      </TabItem>
    </Tabs>
  </TabItem>
</Tabs>

### DeepSeek Coder 2 16B

If your local machine can run a 16B parameter model, then we recommend running DeepSeek Coder 2 16B (e.g. using [Ollama](../model-providers/top-level/ollama.mdx) or [LM Studio](../model-providers/more/lmstudio.mdx)).

<Tabs groupId="config-example">
  {/* HUB_TODO nonexistent blocks */}
  {/* <TabItem value="hub" label="Hub">
    <Tabs groupId="providers">
    <TabItem value="ollama" label="Ollama">
    Add the [Ollama Deepseek Coder 2 16B block](https://hub.continue.dev/explore/models) from the hub
    </TabItem>
    <TabItem value="lmstudio" label="LM Studio">
    Add the [LM Studio Deepseek Coder 2 16B block](https://hub.continue.dev/explore/models) from the hub
    </TabItem>
    </Tabs>
  </TabItem> */}
  <TabItem value="yaml" label="YAML">
    <Tabs groupId="providers">
        <TabItem value="Ollama">
        ```yaml title="config.yaml"
        models:
          - name: DeepSeek Coder 2 16B
            provider: ollama
            model: deepseek-coder-v2:16b
        ```
        </TabItem>
        <TabItem value="LM Studio">
        ```yaml title="config.yaml"
        models:
          - name: DeepSeek Coder 2 16B
            provider: lmstudio
            model: deepseek-coder-v2:16b
        ```
        </TabItem>
    </Tabs>
  </TabItem>
  <TabItem value="json" label="JSON">
    <Tabs groupId="providers">
      <TabItem value="Ollama">
      ```json title="config.json"
      {
        "models": [
          {
            "title": "DeepSeek Coder 2 16B",
            "provider": "ollama",
            "model": "deepseek-coder-v2:16b",
            "apiBase": "http://localhost:11434"
          }
        ]
      }
      ```
      </TabItem>
      <TabItem value="LM Studio">
      ```json title="config.json"
      {
        "models": [
          {
            "title": "DeepSeek Coder 2 16B",
            "provider": "lmstudio",
            "model": "deepseek-coder-v2:16b"
          }
        ]
      }
      ```
      </TabItem>
    </Tabs>
  </TabItem>
</Tabs>

## Other experiences

There are many more models and providers you can use with Chat beyond those mentioned above. Read more [here](../model-roles/chat.mdx)
