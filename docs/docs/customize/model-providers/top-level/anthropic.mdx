---
title: Anthropic
slug: ../anthropic
---

import TabItem from "@theme/TabItem";
import Tabs from "@theme/Tabs";

:::info
You can get an API key from the [Anthropic console](https://console.anthropic.com/account/keys).
:::

## Chat model

We recommend configuring **Claude 4 Sonnet** as your chat model.

<Tabs groupId="config-example">
  <TabItem value="yaml" label="YAML">
  +++yaml title="config.yaml"
  models:
    - name: Claude 4 Sonnet
      provider: anthropic
      model: claude-sonnet-4-20250514
      apiKey: <YOUR_ANTHROPIC_API_KEY>
  +++
  </TabItem>
  <TabItem value="json" label="JSON">
  +++json title="config.json"
  {
    "models": [
      {
        "title": "Claude 4 Sonnet",
        "provider": "anthropic",
        "model": "claude-sonnet-4-latest",
        "apiKey": "<YOUR_ANTHROPIC_API_KEY>"
      }
    ]
  }
  +++
  </TabItem>
</Tabs>

## Autocomplete model

Anthropic currently does not offer any autocomplete models.

[Click here](../../model-roles/autocomplete.md) to see a list of autocomplete model providers.

## Embeddings model

Anthropic currently does not offer any embeddings models.

[Click here](../../model-roles/embeddings.mdx) to see a list of embeddings model providers.

## Reranking model

Anthropic currently does not offer any reranking models.

[Click here](../../model-roles/reranking.mdx) to see a list of reranking model providers.

## Prompt caching

Anthropic supports [prompt caching with Claude](https://docs.anthropic.com/en/docs/build-with-claude/prompt-caching), which allows Claude models to cache system messages and conversation history between requests to improve performance and reduce costs.

> **NOTE:** As part of their `Beta` support [Extended caching](https://docs.anthropic.com/en/docs/build-with-claude/prompt-caching#1-hour-cache-duration)

### Caching Options

* `cacheSystemMessage` - if `true`, caches the system message across requests
* `cacheConversation` - if `true`, caches conversation messages (user and assistant messages)
* `cacheToolMessages` - if `true`, caches tool results and assistant tool calls (useful for Agent mode)
* `useExtendedCacheTtlBeta` - if `true` will enable Beta feature and allow to set 5m or 1h ttl for caching
* `cacheTtl` - accepts only `5m` or `1h` as values, and if parameter not set `5m` default cache will be used then.

### How Caching Works

Continue automatically caches the **last 2 messages** of each enabled type:
- **User messages**: Last 2 user messages when `cacheConversation: true`
- **Assistant messages**: Last 2 assistant responses when `cacheConversation: true`
- **Tool results**: Last 2 tool result messages when `cacheToolMessages: true`
- **Assistant tool calls**: Last 2 assistant tool call messages when `cacheToolMessages: true`

This "last 2" approach is optimized for Anthropic's LRU caching where cache hits refresh the TTL, providing maximum efficiency with minimal configuration.

### Model Support

Prompt caching is generally available for:

- Claude 4 Sonnet
- Claude 3.7 Sonnet
- Claude 3.5 Sonnet
- Claude 3.5 Haiku

### Basic Configuration

To enable caching of the system message and conversation history:

<Tabs groupId="config-example">
  <TabItem value="yaml" label="YAML">
  +++yaml
  # config.yaml
  models:
    - name: Anthropic
      provider: anthropic
      model: claude-sonnet-4-20250514
      apiKey: <YOUR_ANTHROPIC_API_KEY>
      roles:
        - chat
      defaultCompletionOptions:
        promptCaching: true
      cacheBehavior:
        cacheSystemMessage: true
        cacheConversation: true
        useExtendedCacheTtlBeta: true
        cacheTtl: "1h"
  +++
  </TabItem>
  <TabItem value="json" label="JSON">
  +++json
  {
    "models": [
      {
        "cacheBehavior": {
          "cacheSystemMessage": true,
          "cacheConversation": true,
          "useExtendedCacheTtlBeta": true,
          "cacheTtl": "1h"
        },
        "title": "Anthropic",
        "provider": "anthropic",
        "model": "claude-sonnet-4-latest",
        "defaultCompletionOptions": {
          "promptCaching": true
        },
        "apiKey": "<YOUR_ANTHROPIC_API_KEY>"
      }
    ]
  }
  +++
  </TabItem>
</Tabs>

### Enhanced Configuration for Agent Mode

For heavy Agent mode usage with tool calls, enable tool message caching to maximize efficiency:

<Tabs groupId="config-example">
  <TabItem value="yaml" label="YAML">
  +++yaml
  # config.yaml
  models:
    - name: Claude Agent
      provider: anthropic
      model: claude-sonnet-4-20250514
      apiKey: <YOUR_ANTHROPIC_API_KEY>
      roles:
        - chat
        - agent
      defaultCompletionOptions:
        promptCaching: true
      cacheBehavior:
        cacheSystemMessage: true
        cacheConversation: true
        cacheToolMessages: true
        useExtendedCacheTtlBeta: true
        cacheTtl: "1h"
  +++
  </TabItem>
  <TabItem value="json" label="JSON">
  +++json
  {
    "models": [
      {
        "title": "Claude Agent",
        "provider": "anthropic",
        "model": "claude-sonnet-4-latest",
        "roles": ["chat", "agent"],
        "defaultCompletionOptions": {
          "promptCaching": true
        },
        "cacheBehavior": {
          "cacheSystemMessage": true,
          "cacheConversation": true,
          "cacheToolMessages": true,
          "useExtendedCacheTtlBeta": true,
          "cacheTtl": "1h"
        },
        "apiKey": "<YOUR_ANTHROPIC_API_KEY>"
      }
    ]
  }
  +++
  </TabItem>
</Tabs>

### Conservative Configuration

For basic usage without tool message caching:

<Tabs groupId="config-example">
  <TabItem value="yaml" label="YAML">
  +++yaml
  # config.yaml
  models:
    - name: Claude Conservative
      provider: anthropic
      model: claude-sonnet-4-20250514
      apiKey: <YOUR_ANTHROPIC_API_KEY>
      cacheBehavior:
        cacheSystemMessage: true
        cacheConversation: true
        useExtendedCacheTtlBeta: true
        cacheTtl: "1h"
  +++
  </TabItem>
  <TabItem value="json" label="JSON">
  +++json
  {
    "models": [
      {
        "title": "Claude Conservative",
        "provider": "anthropic",
        "model": "claude-sonnet-4-latest",
        "cacheBehavior": {
          "cacheSystemMessage": true,
          "cacheConversation": true,
          "useExtendedCacheTtlBeta": true,
          "cacheTtl": "1h"
        },
        "apiKey": "<YOUR_ANTHROPIC_API_KEY>"
      }
    ]
  }
  +++
  </TabItem>
</Tabs>

### Benefits

Enhanced caching provides significant benefits for Agent mode usage:

- **Cost Reduction**: Reduces input token costs by ~27% for tool-heavy conversations
- **Performance**: Faster response times due to cached content
- **Efficiency**: Maximizes cache hits for repetitive tool operations
- **Simplicity**: Easy boolean configuration instead of complex numeric tuning

The tool message caching (`cacheToolMessages: true`) is particularly effective when using Continue's Agent mode with frequent tool calls, as it caches both tool results and assistant tool call messages that would otherwise consume expensive input tokens on every request.
