---
title: Anthropic
slug: ../anthropic
---

import TabItem from "@theme/TabItem";
import Tabs from "@theme/Tabs";

:::info
You can get an API key from the [Anthropic console](https://console.anthropic.com/account/keys).
:::

## Chat model

We recommend configuring **Claude 4 Sonnet** as your chat model.

<Tabs groupId="config-example">
  <TabItem value="yaml" label="YAML">
  ```yaml title="config.yaml"
  models:
    - name: Claude 4 Sonnet
      provider: anthropic
      model: claude-sonnet-4-20250514
      apiKey: <YOUR_ANTHROPIC_API_KEY>
  ```
  </TabItem>
  <TabItem value="json" label="JSON">
  ```json title="config.json"
  {
    "models": [
      {
        "title": "Claude 4 Sonnet",
        "provider": "anthropic",
        "model": "claude-sonnet-4-latest",
        "apiKey": "<YOUR_ANTHROPIC_API_KEY>"
      }
    ]
  }
  ```
  </TabItem>
</Tabs>

## Autocomplete model

Anthropic currently does not offer any autocomplete models.

[Click here](../../model-roles/autocomplete.md) to see a list of autocomplete model providers.

## Embeddings model

Anthropic currently does not offer any embeddings models.

[Click here](../../model-roles/embeddings.mdx) to see a list of embeddings model providers.

## Reranking model

Anthropic currently does not offer any reranking models.

[Click here](../../model-roles/reranking.mdx) to see a list of reranking model providers.

## Prompt caching

Anthropic supports [prompt caching with Claude](https://docs.anthropic.com/en/docs/build-with-claude/prompt-caching), which allows Claude models to cache system messages and conversation history between requests to improve performance and reduce costs.

> **NOTE:** As part of their `Beta` support [Extended caching](https://docs.anthropic.com/en/docs/build-with-claude/prompt-caching#1-hour-cache-duration) caching TTL can be extended to 1 hour.

**Prompt caching is generally available for:**

- Claude 4 Sonnet
- Claude 3.7 Sonnet
- Claude 3.5 Sonnet
- Claude 3.5 Haiku

### Caching Options

* `cacheSystemMessage` - if `true`, caches the system message across requests
* `cacheConversation` - if `true`, caches conversation messages (user and assistant messages)
* `cacheToolMessages` - if `true`, caches tool results and assistant tool calls (useful for Agent mode)
* `useExtendedCacheTtlBeta` - if `true` will enable Beta feature and allow to set 5m or 1h ttl for caching
* `cacheTtl` - accepts only `5m` or `1h` as values, and if parameter not set `5m` default cache will be used then.
* `cacheDebug` - if `true`, will report into Console information which can help to see and understand usage of cache.

> **NOTE:** `1h` cacheTtl can be set only if `useExtendedCacheTtlBeta` is `true`

### Basic Configuration

To enable caching of the system message and conversation history with basic five minutes caching support:

<Tabs groupId="config-example">
  <TabItem value="yaml" label="YAML">
  ```yaml
  # config.yaml
  models:
    - name: Anthropic
      provider: anthropic
      model: claude-sonnet-4-20250514
      apiKey: <YOUR_ANTHROPIC_API_KEY>
      roles:
        - chat
      defaultCompletionOptions:
        promptCaching: true
      cacheBehavior:
        cacheSystemMessage: true
        cacheConversation: true
  ```
  </TabItem>
  <TabItem value="json" label="JSON">
  ```json
  {
    "models": [
      {
        "cacheBehavior": {
          "cacheSystemMessage": true,
          "cacheConversation": true,
        },
        "title": "Anthropic",
        "provider": "anthropic",
        "model": "claude-sonnet-4-latest",
        "defaultCompletionOptions": {
          "promptCaching": true
        },
        "apiKey": "<YOUR_ANTHROPIC_API_KEY>"
      }
    ]
  }
  ```
  </TabItem>
</Tabs>

### Enhanced Configuration with extended caching support

For heavy Agent mode usage with tool calls, enable tool message caching to maximize efficiency:

<Tabs groupId="config-example">
  <TabItem value="yaml" label="YAML">
  ```yaml
  # config.yaml
  models:
    - name: Claude Agent
      provider: anthropic
      model: claude-sonnet-4-20250514
      apiKey: <YOUR_ANTHROPIC_API_KEY>
      roles:
        - chat
        - agent
      defaultCompletionOptions:
        promptCaching: true
      cacheBehavior:
        cacheSystemMessage: true
        cacheConversation: true
        cacheToolMessages: true
        useExtendedCacheTtlBeta: true
        cacheTtl: "1h"
  ```
  </TabItem>
  <TabItem value="json" label="JSON">
  ```json
  {
    "models": [
      {
        "title": "Claude Agent",
        "provider": "anthropic",
        "model": "claude-sonnet-4-latest",
        "roles": ["chat", "agent"],
        "defaultCompletionOptions": {
          "promptCaching": true
        },
        "cacheBehavior": {
          "cacheSystemMessage": true,
          "cacheConversation": true,
          "cacheToolMessages": true,
          "useExtendedCacheTtlBeta": true,
          "cacheTtl": "1h"
        },
        "apiKey": "<YOUR_ANTHROPIC_API_KEY>"
      }
    ]
  }
  ```
  </TabItem>
</Tabs>

### Benefits

Enhanced caching provides significant benefits for Agent mode usage:

- **Cost Reduction**: Reduces input token costs by ~27% for tool-heavy conversations
- **Performance**: Faster response times due to cached content
- **Efficiency**: Maximizes cache hits for repetitive tool operations
- **Simplicity**: Easy boolean configuration instead of complex numeric tuning

The tool message caching (`cacheToolMessages: true`) is particularly effective when using Continue's Agent mode with frequent tool calls, as it caches both tool results and assistant tool call messages that would otherwise consume expensive input tokens on every request.
