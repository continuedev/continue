---
title: "Metrics"
description: "Agent observability for Cloud Agents: track activity, success rates, PR outcomes, cost, and workflow reliability across your repositories."
---

<Info>
Metrics provides Agent observability for Cloud Agents and automated workflows. Agent observability answers a simple question: *What are my AI agents doing, and is it working?* Use Metrics to monitor agent activity, understand human intervention, measure success rates, and evaluate the cost and impact of AI-driven work across your repositories.
</Info>

## What Metrics Show About Your Cloud Agents

Continue’s Metrics give you **operational observability for AI agents**, similar to how traditional observability tools provide visibility into services, jobs, and pipelines.

Instead of logs and latency, agent observability focuses on:
- Runs and execution frequency  
- Success vs. human intervention  
- Pull request outcomes  
- Cost per run and per workflow  

<AccordionGroup>

  <Accordion title="Agent Activity & Execution Volume">
    Understand **when and how often your agents run**.
    - See which Cloud Agents are running most often  
    - Spot spikes, trends, or recurring failures  
    - Monitor automated Workflows in production  
  </Accordion>

  <Accordion title="Agent Success & Outcome Metrics">
    Measure **whether agents produce usable results**.
    - **Total runs**  
    - **PR creation rate**  
    - **PR status** (open, merged, closed, failed)  
    - **Success vs. intervention rate**  
  </Accordion>

  <Accordion title="Workflow Reliability & Impact">
    Evaluate **automated agent workflows in production**.
    - Which Workflows generate the most work  
    - Completion and success rates  
    - Signals that a Workflow needs refinement or guardrails  
  </Accordion>

</AccordionGroup>

## Available Charts

The Metrics page displays time-series charts for analyzing agent performance over configurable date ranges.

### Outcome Breakdown

Stacked bar chart showing daily session outcomes:

- **Accept** – User accepted the suggestion or PR was merged
- **Reject** – User rejected the suggestion
- **Skip** – PR was closed without merging, or session completed with uncommitted changes
- **No Action** – Session completed without creating commits or PRs

Filter by workflow to see outcomes for specific agent configurations.

### PR Velocity

Line chart tracking average time from session creation to merge (in hours). Shows both average and median values per day to help identify bottlenecks in your review process.

### PR Volume

Bar chart showing the number of PRs created per day. Use this to understand agent activity levels and spot trends in automation usage.

### Failure Rate

Line chart displaying the daily percentage of sessions that resulted in reject or skip outcomes. A rising failure rate may indicate agents need better prompts, rules, or tool configurations.

## Why Metrics Matter

<CardGroup cols={2}>

  <Card title="Improve Agent Reliability" icon="line-chart">
    Identify which Agents need better rules, tools, or prompts.
  </Card>

  <Card title="Measure Automation Value" icon="bar-chart">
    See how much work your automated Workflows are completing across your repos.
  </Card>

</CardGroup>