---
title: 重排序角色
description: 重排序模型角色
keywords: [重排序, 模型, 角色]
sidebar_position: 6
---

import TabItem from "@theme/TabItem";
import Tabs from "@theme/Tabs";

"重排序模型" 是训练为采用两部分文本（通常一个用户问题和一个文档）并返回 0 和 1 之间的相关性得分，估算文档回答问题的可用性。重排序通常远小于 LLM ，在比较中非常快速和便宜。

在 Continue 中，重排序用在 [@Codebase](../deep-dives/codebase.mdx) ，为了在向量搜索之后选择最想关的代码片断。

## 推荐的重排序模型

如果你有使用任何模型的能力，我们推荐 Voyage AI 的 `rerank-2` ，它独立列在下面，和其他的重排序选项一起。

### Voyage AI

Voyage AI 提供代码最好的重排序模型， 使用他们的 `rerank-2` 模型。在从 [这里](https://www.voyageai.com/) 获取 API key 后，你可以像这样配置：

<Tabs groupId="hub-config-example">
  <TabItem value="hub" label="Hub">
  [Voyage Rerank 2 Block](https://hub.continue.dev/voyageai/rerank-2)
  </TabItem>
  <TabItem value="yaml" label="YAML">
  ```yaml title="config.yaml"
  models:
    - name: My Voyage Reranker
      provider: voyage
      apiKey: <YOUR_VOYAGE_API_KEY>
      model: rerank-2
      roles:
        - rerank
  ```
  </TabItem>
  <TabItem value="json" label="JSON">
  ```json title="config.json"
  {
    "reranker": {
        "name": "voyage",
        "params": {
            "model": "rerank-2",
            "apiKey": "<YOUR_VOYAGE_API_KEY>"
        }
    }
  }
  ```
  </TabItem>
</Tabs>

### Cohere

在 [这里](https://docs.cohere.com/docs/rerank-2) 查看 Cohere 对于重排序的文档。

<Tabs groupId="config-example">
  {/* HUB_TODO block doesn't exist */}
  {/* <TabItem value="hub" label="Hub">
  [Cohere Reranker English v3](https://hub.continue.dev/)
  </TabItem> */}
  <TabItem value="yaml" label="YAML">
  ```yaml title="config.yaml"
  models:
    - name: Cohere Reranker
      provider: cohere
      model: rerank-english-v3.0
      apiKey: <YOUR_COHERE_API_KEY>
      roles:
        - rerank
  ```
  </TabItem>
  <TabItem value="json" label="JSON">
  ```json title="config.json"
  {
    "reranker": {
        "name": "cohere",
        "params": {
          "model": "rerank-english-v3.0",
          "apiKey": "<YOUR_COHERE_API_KEY>"
        }
    }
  }
  ```
  </TabItem>
</Tabs>

### LLM

如果你只能访问一个单独的 LLM ，那么你可以使用它作为重排器。这是不推荐的，除非真正需要，因为它将会更昂贵，并且仍然比上面这些特殊任务训练的模型慢。注意，如果你使用本地的模型，这将不会工作，例如使用 Ollama ，因为会创建太多的并发请求。

<Tabs groupId="config-example">
  {/* HUB_TODO block doesn't exist */}
  {/* <TabItem value="hub" label="Hub">
  [GPT-4o LLM Reranker Block](https://hub.continue.dev/)
  </TabItem> */}
  <TabItem value="yaml" label="YAML">
  ```yaml title="config.yaml"
  models:
    - name: LLM Reranker
      provider: openai
      model: gpt-4o
      roles:
        - rerank
  ```
  </TabItem>
  <TabItem value="json" label="JSON">
  ```json title="config.json"
  {
    "reranker": {
        "name": "llm",
        "params": {
            "modelTitle": "My Model Title"
        }
    }
  }
  ```
  </TabItem>
</Tabs>

`"modelTitle"` 字段必须匹配在你的 `config.json` 中 "models" 列表中的一个模型。

### 文本嵌入推理

[Hugging Face 文本嵌入推理](https://huggingface.co/docs/text-embeddings-inference/en/index) 允许你托管自己的 [重排器端点](https://huggingface.github.io/text-embeddings-inference/#/Text%20Embeddings%20Inference/rerank) 。你可以像这样配置你的重排器：

<Tabs groupId="config-example">
  {/* HUB_TODO */}
  {/* <TabItem value="hub" label="Hub">
  [HuggingFace TEI Reranker block](https://hub.continue.dev/)
  </TabItem> */}
    <TabItem value="yaml" label="YAML">
    ```yaml title="config.yaml"
    models:
      - name: Huggingface-tei Reranker
        provider: huggingface-tei
        apiBase: http://localhost:8080
        apiKey: <YOUR_TEI_API_KEY>
        roles:
          - rerank
    ```
    </TabItem>
    <TabItem value="json" label="JSON">
    ```json title="config.json"
    {
      "reranker": {
        "name": "huggingface-tei",
        "params": {
          "apiBase": "http://localhost:8080",
          "apiKey": "<YOUR_TEI_API_KEY>"
        }
      }
    }
    ```
    </TabItem>
</Tabs>
