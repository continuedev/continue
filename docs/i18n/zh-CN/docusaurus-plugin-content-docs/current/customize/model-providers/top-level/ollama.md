---
title: Ollama
slug: ../ollama
---

Ollama 是一个开源工具，允许在他们自己的计算机上本地运行大语言模型 (LLMs) 。为了使用 Ollama ，你可以安装它 [这里](https://ollama.ai/download) 并下载你想要运行的模型，使用 `ollama run` 命令。

## 聊天模型

我们推荐配置 **Llama3.1 8B** 作为你的聊天模型。

```json title="config.json"
{
  "models": [
    {
      "title": "Llama3.1 8B",
      "provider": "ollama",
      "model": "llama3.1:8b"
    }
  ]
}
```

## 自动补全模型

我们推荐配置 **Qwen2.5-Coder 1.5B** 作为你的自动补全模型。

```json title="config.json"
{
  "tabAutocompleteModel": {
    "title": "Qwen2.5-Coder 1.5B",
    "provider": "ollama",
    "model": "qwen2.5-coder:1.5b-base"
  }
}
```

## 嵌入模型

我们推荐配置 **Nomic Embed Text** 作为你的嵌入模型。

```json title="config.json"
{
  "embeddingsProvider": {
    "provider": "ollama",
    "model": "nomic-embed-text"
  }
}
```

## 重排序模型

Ollama 当前没有提供任何重排序模型。

[点击这里](../../model-types/reranking.md) 查看重排序模型提供者列表。

## 使用一个远程实例

为了配置一个 Ollama 的远程实例，在 config.json 中添加 `"apiBase"` 属性到你的模型：

```json title="config.json"
{
  "models": [
    {
      "title": "Llama3.1 8B",
      "provider": "ollama",
      "model": "llama3.1:8b",
      "apiBase": "http://<my endpoint>:11434"
    }
  ]
}
```
