---
title: "Using Ollama with Continue: A Developer's Guide"
description: "This comprehensive guide will walk you through setting up Ollama with Continue for powerful local AI development. Ollama allows you to run large language models locally on your machine, providing privacy, control, and offline capabilities for AI-assisted coding."
---

## Prerequisites[​](#prerequisites "Direct link to Prerequisites")

Before getting started, ensure your system meets these requirements:

* Operating System: macOS, Linux, or Windows
* RAM: Minimum 8GB (16GB+ recommended)
* Storage: At least 10GB free space
* Continue extension installed

## Installation Steps[​](#installation-steps "Direct link to Installation Steps")

### Step 1: Install Ollama[​](#step-1-install-ollama "Direct link to Step 1: Install Ollama")

Choose the installation method for your operating system:

```
# macOSbrew install ollama# Linuxcurl -fsSL https://ollama.ai/install.sh | sh# Windows# Download from ollama.ai
```

### Step 2: Download Models[​](#step-2-download-models "Direct link to Step 2: Download Models")

After installing Ollama, download the models you want to use. Here are some popular options:

```
# Popular models for developmentollama pull llama2ollama pull codellamaollama pull mistral
```

## Configuration[​](#configuration "Direct link to Configuration")

Configure Continue to work with your local Ollama instance:

### Continue Configuration[​](#continue-configuration "Direct link to Continue Configuration")

```json
{  "models": [    {      "title": "Ollama",      "provider": "ollama",      "model": "llama2",      "apiBase": "http://localhost:11434"    }  ]}
```

### Advanced Settings[​](#advanced-settings "Direct link to Advanced Settings")

For optimal performance, consider these advanced configuration options:

* Memory optimization
* GPU acceleration
* Custom model parameters
* Performance tuning

## Best Practices[​](#best-practices "Direct link to Best Practices")

### Model Selection[​](#model-selection "Direct link to Model Selection")

Choose models based on your specific needs:

1. **Code Generation**: Use CodeLlama or Mistral
2. **Chat**: Llama2 or Mistral
3. **Specialized Tasks**: Domain-specific models

### Performance Optimization[​](#performance-optimization "Direct link to Performance Optimization")

To get the best performance from Ollama:

* Monitor system resources
* Adjust context window size
* Use appropriate model sizes
* Enable GPU acceleration when available

## Troubleshooting[​](#troubleshooting "Direct link to Troubleshooting")

### Common Issues[​](#common-issues "Direct link to Common Issues")

Here are solutions to common problems you might encounter:

#### Connection Problems[​](#connection-problems "Direct link to Connection Problems")

* Check Ollama service status
* Verify port availability
* Review firewall settings

#### Performance Issues[​](#performance-issues "Direct link to Performance Issues")

* Insufficient RAM
* Model too large for system
* GPU compatibility

### Solutions[​](#solutions "Direct link to Solutions")

Try these solutions in order:

1. Restart Ollama service
2. Clear model cache
3. Update to latest version
4. Check system requirements

## Example Workflows[​](#example-workflows "Direct link to Example Workflows")

### Code Generation[​](#code-generation "Direct link to Code Generation")

```
# Example: Generate a FastAPI endpointdef create_user_endpoint():    # Continue will help generate the implementation    pass
```

### Code Review[​](#code-review "Direct link to Code Review")

Use Continue with Ollama to:

* Analyze code quality
* Suggest improvements
* Identify potential bugs
* Generate documentation

## Conclusion[​](#conclusion "Direct link to Conclusion")

Ollama with Continue provides a powerful local development environment for AI-assisted coding. You now have complete control over your AI models, ensuring privacy and enabling offline development workflows.

***

*This guide is based on Ollama v0.1.x and Continue v0.8.x. Please check for updates regularly.*

[Edit this page](https://github.com/continuedev/continue/tree/main/docs/docs/guides/ollama-guide.mdx)
